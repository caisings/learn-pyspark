{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"Superstore Analysis\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "__all__ = [\n",
    "    \"DataType\", \"NullType\", \"StringType\", \"BinaryType\", \"BooleanType\", \"DateType\",\n",
    "    \"TimestampType\", \"DecimalType\", \"DoubleType\", \"FloatType\", \"ByteType\", \"IntegerType\",\n",
    "    \"LongType\", \"ShortType\", \"ArrayType\", \"MapType\", \"StructField\", \"StructType\"]\n",
    "\"\"\"\n",
    "#create df with custom schema\n",
    "schema = StructType([\n",
    "\tStructField(\"row_id\", StringType(), True),\n",
    "\tStructField(\"order_id\", StringType(), True),\n",
    "\tStructField(\"order_date\", StringType(), True),\n",
    "\tStructField(\"ship_date\", StringType(), True),\n",
    "\tStructField(\"ship_mode\", StringType(), True),\n",
    "\tStructField(\"customer_id\", StringType(), True),\n",
    "\tStructField(\"customer_name\", StringType(), True),\n",
    "\tStructField(\"segment\", StringType(), True),\n",
    "\tStructField(\"country\", StringType(), True),\n",
    "\tStructField(\"city\", StringType(), True),\n",
    "\tStructField(\"state\", StringType(), True),\n",
    "\tStructField(\"postal_code\", StringType(), True),\n",
    "\tStructField(\"region\", StringType(), True),\n",
    "\tStructField(\"product_id\", StringType(), True),\n",
    "\tStructField(\"category\", StringType(), True),\n",
    "\tStructField(\"sub_category\", StringType(), True),\n",
    "\tStructField(\"product_name\", StringType(), True),\n",
    "\tStructField(\"sales\", DoubleType(), True),\n",
    "\tStructField(\"quantity\", IntegerType(), True),\n",
    "\tStructField(\"discount\", DoubleType(), True),\n",
    "\tStructField(\"profit\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "#create df with schema define\n",
    "superstore_df = spark.read.csv(\"D:/Work/training/sample_training/Superstores.csv\", sep=\"|\", schema=schema, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=read%20csv\n",
    "\n",
    "#create df with automatically define schema\n",
    "superstore_df = spark.read.csv(\"D:/Work/training/sample_training/Superstores.csv\", sep=\"|\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Row ID', 'int'),\n",
       " ('Order ID', 'string'),\n",
       " ('Order Date', 'string'),\n",
       " ('Ship Date', 'string'),\n",
       " ('Ship Mode', 'string'),\n",
       " ('Customer ID', 'string'),\n",
       " ('Customer Name', 'string'),\n",
       " ('Segment', 'string'),\n",
       " ('Country', 'string'),\n",
       " ('City', 'string'),\n",
       " ('State', 'string'),\n",
       " ('Postal Code', 'int'),\n",
       " ('Region', 'string'),\n",
       " ('Product ID', 'string'),\n",
       " ('Category', 'string'),\n",
       " ('Sub-Category', 'string'),\n",
       " ('Product Name', 'string'),\n",
       " ('Sales', 'double'),\n",
       " ('Quantity', 'int'),\n",
       " ('Discount', 'double'),\n",
       " ('Profit', 'double')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "superstore_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Order Date|\n",
      "+----------+\n",
      "| 11/8/2016|\n",
      "| 11/8/2016|\n",
      "| 6/12/2016|\n",
      "|10/11/2015|\n",
      "|10/11/2015|\n",
      "|  6/9/2014|\n",
      "|  6/9/2014|\n",
      "|  6/9/2014|\n",
      "|  6/9/2014|\n",
      "|  6/9/2014|\n",
      "|  6/9/2014|\n",
      "|  6/9/2014|\n",
      "| 4/15/2017|\n",
      "| 12/5/2016|\n",
      "|11/22/2015|\n",
      "|11/22/2015|\n",
      "|11/11/2014|\n",
      "| 5/13/2014|\n",
      "| 8/27/2014|\n",
      "| 8/27/2014|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select dataframe column\n",
    "superstore_df.select(\"Order Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#python cast to date\n",
    "def castToDate(dates):\n",
    "    new_dates = datetime.datetime.strptime(dates, '%d/%m/%Y').date()\n",
    "    return new_dates\n",
    "\n",
    "#to convert string use .strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, to_date, year, month, dayofmonth\n",
    "\n",
    "#Convert string dataframe to date dataframe with specific format\n",
    "temp_df = superstore_df\\\n",
    "    .withColumn('Order Date', lit(to_date(superstore_df['Order Date'], 'mm/dd/yyyy')))\\\n",
    "    .withColumn('Ship Date', lit(to_date(superstore_df['Ship Date'], 'mm/dd/yyyy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Order Date=datetime.date(2016, 1, 8)),\n",
       " Row(Order Date=datetime.date(2016, 1, 8)),\n",
       " Row(Order Date=datetime.date(2016, 1, 12))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.select(\"Order Date\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "#function to Concat column in dataframe\n",
    "def myConcat(*cols):\n",
    "    return F.concat(*[F.coalesce(c, F.lit(\"*\")) for c in cols])\n",
    "\n",
    "#Or you can use:\n",
    "#new_df = temp_df.withColumn('Period', myConcat.concat(year(\"Order Date\"), month(\"Order Date\"))).show()\n",
    "#new_df = temp_df.withColumn('Period', F.concat(year(\"Order Date\"), month(\"Order Date\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dd = (dd.withColumn('month', F.when(F.length(F.col('month')) == 1, F.concat(F.lit('0'), F.col('month'))).otherwise(F.col('month')))\n",
    "        .withColumn('date', F.when(F.length(F.col('date')) == 1, F.concat(F.lit('0'), F.col('date'))).otherwise(F.col('date')))\n",
    "        .withColumn('hhmm', F.when(F.length(F.col('hhmm')) == 1, F.concat(F.lit('000'), F.col('hhmm')))\n",
    "                             .when(F.length(F.col('hhmm')) == 2, F.concat(F.lit('00'), F.col('hhmm')))\n",
    "                             .when(F.length(F.col('hhmm')) == 3, F.concat(F.lit('0'), F.col('hhmm')))\n",
    "                             .otherwise(F.col('hhmm')))\n",
    "        .withColumn('time', F.to_timestamp(F.concat(*dd.columns), format='yyyyMMddHHmm'))\n",
    "     )\n",
    "\"\"\"\n",
    "#F.when(F.length(F.col(temp_df.select(month('Order Date')))) == 1, F.concat(F.lit('0'), F.col(temp_df.select(month('Order Date'))))).otherwise(F.col(temp_df.select(month(\"Order Date\"))))\n",
    "\n",
    "new_df = temp_df.withColumn('Period', F.concat(year(\"Order Date\"), month(\"Order Date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Row ID=1, Order ID='CA-2016-152156', Order Date=datetime.date(2016, 1, 8), Ship Date=datetime.date(2016, 1, 11), Ship Mode='Second Class', Customer ID='CG-12520', Customer Name='Claire Gute', Segment='Consumer', Country='United States', City='Henderson', State='Kentucky', Postal Code=42420, Region='South', Product ID='FUR-BO-10001798', Category='Furniture', Sub-Category='Bookcases', Product Name='Bush Somerset Collection Bookcase', Sales=261.96, Quantity=2, Discount=0.0, Profit=41.9136, Period='20161')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|Period|        sum(Sales)|\n",
      "+------+------------------+\n",
      "| 20151|470532.50899999985|\n",
      "| 20141| 484247.4981000009|\n",
      "| 20161| 609205.5980000008|\n",
      "| 20171| 733215.2551999999|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.groupBy(\"Period\").agg({\"Sales\":\"sum\"}).alias(\"sum_sales\").orderBy(\"sum_sales.sum(Sales)\").show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
